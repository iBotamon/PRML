{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目標値が $t\\in\\{0,1\\}$ であり, $y(\\mathbf{x}, \\mathbf{w})$ が $p(t=1|\\mathbf{x})$ を出力するような2クラス分類モデルを構成したいとき:\n",
    "\n",
    "### 一般線形モデル (§4)\n",
    "\n",
    "- $y=f(\\mathbf{w}^{T}\\mathbf{x})$ とおいて, $\\mathbf{w}$を最適化する. $f$は非線形活性化関数.\n",
    "\n",
    "#### 一般線形モデル > 識別関数 (§4.1)\n",
    "- $f$は恒等写像.\n",
    "\n",
    "##### 一般線形モデル > 識別関数 > 最小二乗法 (§4.1.1 - 4.1.3)\n",
    "- 二乗和誤差関数 $\\displaystyle E(W)=\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^{T}(XW-T)\\}$ を最小化する.\n",
    "- $W=(X^{T}X)^{-1}X^{T}T = X^{\\dagger}T$\n",
    "- $W\\mathbf{x}\\ge0$のとき$C_{1}$, $W\\mathbf{x}<0$のとき$C_{0}$と判定\n",
    "    - あまりうまくいかない\n",
    "    - そもそも最小二乗法とは目的変数の条件付き確率分布にガウス分布を仮定したときの最尤推定\n",
    "    - ここでは目的変数は二値変数であり, ガウス分布とはかけ離れているので当然のこと\n",
    "\n",
    "##### 一般線形モデル > 識別関数 > フィッシャーの線形判別 (§4.1.4 - 4.1.6)\n",
    "- $\\mathbf{w}\\propto (\\mathbf{m}_{1}-\\mathbf{m}_{0})$\n",
    "- $\\mathbf{w}^{T}\\mathbf{x}\\ge -w_{0}$のとき$C_{1}$, $\\mathbf{w}^{T}\\mathbf{x}<-w_{0}$のとき$C_{0}$と判定\n",
    "    - しきい値$w_{0}$は$p(y|C_{k})$をモデル化して最尤推定などで求める\n",
    "\n",
    "#### 一般線形モデル > 識別モデル (§4.3)\n",
    "\n",
    "##### 一般線形モデル > 識別モデル > ロジスティック回帰  (§4.3.1 - 4.3.3)\n",
    "- $f$はロジスティックシグモイド. 理由は誤差関数の勾配が簡潔に書けるため(正準連結関数).\n",
    "- 最尤推定する場合の誤差関数(負の対数尤度比)はcross-entropy: $\\displaystyle E(\\mathbf{w})=-\\sum_{n=1}^{N}\\left(t_{n}\\ln y_{n}+(1-t_{n})\\ln(1-y_{n})\\right)$\n",
    "    - この $E(\\mathbf{w})$ は解析的には最小化できない.\n",
    "    - $E(\\mathbf{w})$ の勾配は $\\displaystyle\\nabla_{\\mathbf{w}}E(\\mathbf{w})=\\sum_{n=1}^{N}(y_{n}-t_{n})\\boldsymbol{\\phi}$  ($\\boldsymbol{\\phi}$:固定された基底関数)\n",
    "    - ニュートン・ラフソン法によって$\\mathbf{w}$を更新する方法は $\\mathbf{w}^{\\mathrm{new}}=\\mathbf{w}^{\\mathrm{old}}-H^{-1}\\nabla_{\\mathbf{w}}E(\\mathbf{w})=(\\Phi^{T}R\\Phi)^{-1}\\Phi^{T}R\\mathbf{z}$\n",
    "        - $\\Phi$ は $\\boldsymbol{\\phi}_{n}^{T}$ を行ベクトルにもつ$N\\times D$行列, $R$は $R_{nn}=y_{n}(1-y_{n})$ なる対角行列\n",
    "        - $\\mathbf{z}$ は $N$次元ベクトルで $\\mathbf{z}=\\Phi \\mathbf{w}^{\\mathrm{old}} - R^{-1}(\\textsf{y}-\\textsf{t})$\n",
    "    - 正則化しないと過学習するので注意.\n",
    "\n",
    "##### 一般線形モデル > 識別モデル > プロビット回帰  (§4.3.5)\n",
    "\n",
    "##### 一般線形モデル > 識別モデル > ベイズロジスティック回帰  (§4.5)\n",
    "\n",
    "#### 一般線形モデル >  確率的生成モデル (§4.2)\n",
    "- $f$はロジスティックシグモイド.\n",
    "- 理由は $\\displaystyle p(t=1|\\mathbf{x})=\\displaystyle p(C_{1}|\\mathbf{x})=\\frac{1}{1+\\displaystyle\\frac{p(\\mathbf{x}|C_{1})p(C_{1})}{p(\\mathbf{x}|C_{0})p(C_{0})}}=\\sigma(a),\\:\\:\\:\\:a=\\ln\\frac{p(\\mathbf{x}|C_{1})p(C_{1})}{p(\\mathbf{x}|C_{0})p(C_{0})}$ とおけるため.\n",
    "- $a$を構成する事前確率を推定すればよい.\n",
    "\n",
    "##### 確率的生成モデル > 入力が連続値の場合  (§4.2.1 - 4.2.2)\n",
    "\n",
    "- 一般に $p(\\mathbf{x}|C_{k})$ が正準形指数型分布族のメンバー $(p(\\mathbf{x}|\\boldsymbol\\lambda_{k})=h(\\mathbf{x})g(\\boldsymbol\\lambda_{k})\\exp(\\boldsymbol\\lambda_{k}^{T}\\mathbf{x}))$ であるとき\n",
    "    - $s$をクラス間で共有された尺度パラメーターとすると $a=\\displaystyle\\frac{1}{s}(\\boldsymbol\\lambda_{1}-\\boldsymbol\\lambda_{0})^{T}\\mathbf{x}+\\ln\\frac{g(\\boldsymbol\\lambda_{1})p(C_{1})}{g(\\boldsymbol\\lambda_{0})p(C_{0})}$\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\displaystyle\\frac{1}{s}(\\boldsymbol\\lambda_{1}-\\boldsymbol\\lambda_{0}),\\:\\:\\:\\: w_{0}=\\ln\\frac{g(\\boldsymbol\\lambda_{1})p(C_{1})}{g(\\boldsymbol\\lambda_{0})p(C_{0})}$\n",
    "    \n",
    "        - あとは $\\boldsymbol\\lambda_{1},\\:\\boldsymbol\\lambda_{0},\\:p(C_{1}),\\:p(C_{0})$ を推定すればよい\n",
    "\n",
    "\n",
    "- とくに $p(\\mathbf{x}|C_{k})$ がガウス分布 $\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_{k},\\Sigma)$ のとき\n",
    "\n",
    "    - $\\displaystyle a=\\left(\\Sigma^{-1}(\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0})\\right)^{T}\\mathbf{x}-\\frac{1}{2}\\left(\n",
    "\\boldsymbol{\\mu}_{1}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{0}\n",
    "\\right)\n",
    "+\n",
    "\\ln \\frac{p(C_{1})}{p(C_{0})}$\n",
    "\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\Sigma^{-1}(\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0}),\\:\\:\\:\\:\n",
    "w_{0} = -\\frac{1}{2}\\left(\n",
    "\\boldsymbol{\\mu}_{1}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{0}\n",
    "\\right)\n",
    "+\n",
    "\\ln \\frac{p(C_{1})}{p(C_{0})}$\n",
    "\n",
    "        - 最尤推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}}{N},\\:\\:\\:\\boldsymbol{\\mu}_{k}=\\frac{1}{N_{k}}\\sum_{n=1}^{N}t_{n}\\mathbf{x}_{k},\\:\\:\\:\\Sigma=\\frac{1}{N}\\sum_{k}\\sum_{n\\in C_{k}}(\\mathbf{x}_{k}-\\boldsymbol{\\mu}_{k})(\\mathbf{x}_{k}-\\boldsymbol{\\mu}_{k})^{T}$\n",
    "        - MAP推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}(N_{k}+1)}{N(N+1)}$\n",
    "    \n",
    "##### 確率的生成モデル > 入力が離散値の場合  (§4.2.3)\n",
    "\n",
    "- 特徴値$x_{i}$が$(0,1)$の離散値をとり, $i\\ne j$に対して$x_{i},x_{j}$が独立なとき $\\displaystyle \\left( p(\\mathbf{x}|C_{k})=\\prod_{i=1}^{D} \\mu_{ki}^{x_{i}}(1-\\mu_{ki})^{1-x_{i}}\\right)$\n",
    "\n",
    "    - $\\displaystyle a = \\sum_{i=1}^{D}\\ln\\frac{\\mu_{1i}(1-\\mu_{0i})}{\\mu_{0i}(1-\\mu_{1i})}x_{i}+\\sum_{i=1}^{D}\\ln\\frac{1-\\mu_{1i}}{1-\\mu_{0i}}+\\ln\\frac{p(C_{1})}{p(C_{0})}$\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\biggl\\{\\ln\\frac{\\mu_{1i}(1-\\mu_{0i})}{\\mu_{0i}(1-\\mu_{1i})}\\biggr\\},\\:\\:\\:\\:w_{0}=\\sum_{i=1}^{D}\\ln\\frac{1-\\mu_{1i}}{1-\\mu_{0i}}+\\ln\\frac{p(C_{1})}{p(C_{0})}$\n",
    "        - 最尤推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}}{N},\\:\\:\\:\\:\\mu_{ki}=\\frac{N_{ki}}{N_{k}}$\n",
    "        - MAP推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}(N_{k}+1)}{N(N+1)},\\:\\:\\:\\:\\mu_{ki}=\\frac{N_{ki}(N_{ki}+1)}{N_{k}(N_{k}+1)}$\n",
    "- 特徴値$x_{i}$が$M_{i}$次元のone-hot vector $\\boldsymbol{\\phi}_{i}$で表現でき, $i\\ne j$に対して$x_{i},x_{j}$が独立なとき $\\displaystyle \\left( p(\\mathbf{x}|C_{k})=\\prod_{i=1}^{D}\\prod_{j=1}^{M_{i}} \\mu_{kij}^{\\phi_{ij}} \\right)$\n",
    "\n",
    "    - $\\displaystyle a = \\sum_{i=1}^{D}\\sum_{j=1}^{M_{i}}\\ln\\frac{\\mu_{1ij}}{\\mu_{0ij}}\\phi_{ij}+\\ln\\frac{p(C_{1})}{p(C_{0})} = \\sum_{i=1}^{D}\\mathbf{m}_{i}^{T}\\boldsymbol{\\phi}_{i}+\\ln\\frac{p(C_{1})}{p(C_{0})}\\:\\:\\:\\:\\left(\\mathbf{m}_{i}=\\biggl\\{ \\ln\\frac{\\mu_{1ij}}{\\mu_{0ij}}\\biggr\\}\\right)$\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\biggl\\{\\mathbf{m}_{i}\\biggr\\},\\:\\:\\:\\:w_{0}=\\ln\\frac{p(C_{1})}{p(C_{0})}$\n",
    "        - 最尤推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}}{N},\\:\\:\\:\\:\\mu_{kij}=\\frac{N_{kij}}{N_{ki}}$\n",
    "        - MAP推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}(N_{k}+1)}{N(N+1)},\\:\\:\\:\\:\\mu_{kij}=\\frac{N_{kij}(N_{kij}+1)}{N_{ki}(N_{ki}+1)}$\n",
    "        \n",
    "        \n",
    "### フィードフォワードネットワーク (§5)\n",
    "\n",
    "- $y=f(\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x}))$ とおいて, $\\mathbf{w}$と$\\boldsymbol{\\phi}$を最適化する. $f$は非線形活性化関数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目標値が $t\\in\\{-1,1\\}$ であり, $y(\\mathbf{x}, \\mathbf{w})$ が $-1\\le y \\le 1$ を出力するような2クラス分類モデルを構成したいとき:\n",
    "\n",
    "- 活性化関数をロジスティックシグモイドから $\\tanh$ に変更する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
