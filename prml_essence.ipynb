{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 準備\n",
    "\n",
    "## 0-1. 単変量ガウス分布 (§2.3)\n",
    "\n",
    "### 0-1-1. 最尤推定とベイズ推論 (§2.3.4 - 2.3.6)\n",
    "- $\\displaystyle p(x)=\\mathcal{N}(x\\:|\\:\\mu,\\sigma^{2}) = \\mathcal{N}(x\\:|\\:\\mu,\\lambda^{-1})$ とおくとき,\n",
    "\n",
    "    - 最尤推定\n",
    "        - $\\displaystyle\\mu_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^{N}x_{n}\\;\\;\\;\\left(\\mathbb{E}\\left[\\mu_{\\mathrm{ML}}\\right]=\\mu\\right)$\n",
    "        - $\\displaystyle\\sigma^{2}_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^{N}(x_{n}-\\mu_{\\mathrm{ML}})^{2}\\;\\;\\;\\left(\\mathbb{E}\\left[\\sigma^{2}_{\\mathrm{ML}}\\right]=\\frac{N-1}{N}\\sigma^{2}\\right)$\n",
    "        \n",
    "    - 精度は既知で平均が未知のときのベイズ推論\n",
    "        - 平均の事前分布: $p(\\mu)=\\mathcal{N}\\left(\\mu\\:|\\:\\mu_{0},\\lambda_{0}^{-1}\\right)$\n",
    "        - 平均の事後分布: $\\displaystyle p(\\mu|\\mathbf{X})=\\mathcal{N}\\left(\\mu\\:\\left|\\:\\frac{N\\lambda_{0}^{-1}\\mu_{\\mathrm{ML}}+\\lambda^{-1}\\mu_{0}}{N\\lambda_{0}^{-1}+\\lambda^{-1}},\\:\\:\\left(N\\lambda+\\lambda_{0}\\right)^{-1}\\right.\\right)$\n",
    "        - 平均の点推定値: $\\displaystyle\\mu_{\\mathrm{MAP}}=\\frac{N\\lambda_{0}^{-1}\\mu_{\\mathrm{ML}}+\\lambda^{-1}\\mu_{0}}{N\\lambda_{0}^{-1}+\\lambda^{-1}}$\n",
    "        \n",
    "    - 平均は既知で精度が未知のときのベイズ推論       \n",
    "        - 精度の事前分布: $p(\\lambda)=\\mathrm{Gam}\\left(\\lambda\\,|\\,a_{0},b_{0}\\right)$ (ガンマ分布)\n",
    "        - 精度の事後分布: $\\displaystyle p(\\lambda|\\mathbf{X})=\\mathrm{Gam}\\left(\\lambda\\,\\left|\\,a_{0}+\\frac{N}{2},\\:\\: b_{0}+\\frac{1}{2}\\sigma^{2}_{\\mathrm{ML}}\\right.\\right)$\n",
    "        - 精度の点推定値: $\\displaystyle \\lambda_{\\mathrm{MAP}}=\\frac{a_{0}+\\frac{N}{2}-1}{b_{0}+\\frac{1}{2}\\sigma^{2}_{\\mathrm{ML}}}$\n",
    "    - 平均も精度も未知のときのベイズ推論       \n",
    "        - 平均と精度の事前分布: $p(\\mu,\\lambda\\,|\\,\\mu_{0},\\beta,a_{0},b_{0})=\\mathcal{N}\\left(\\mu\\:|\\:\\mu_{0},(\\beta\\lambda)^{-1}\\right)\\mathrm{Gam}\\left(\\lambda\\,|\\,a_{0},b_{0}\\right)$ (ガウス-ガンマ分布)\n",
    "        - 平均と精度の事後分布: $\\displaystyle p(\\mu,\\lambda\\,|\\,\\mathbf{X},\\mu_{0},\\beta,a_{0},b_{0})=\\mathcal{N}\\left(\\mu\\:\\left|\\:\\mu_{0}+\\frac{N}{\\beta}\\mu_{\\mathrm{ML}},\\:(\\beta\\lambda)^{-1}\\right.\\right)\\mathrm{Gam}\\left(\\lambda\\,\\left|\\,a_{0}+\\frac{N}{2},\\:b_{0}+\\frac{N}{2}\\sigma^{2}_{\\mathrm{ML}}+N\\mu_{\\mathrm{ML}}^{2}\\right.\\right)$\n",
    "\n",
    "## 0-2. 多変量ガウス分布 (§2.3)\n",
    "- $\\displaystyle p(\\mathbf{x})=\\mathcal{N}(\\mathbf{x}\\:|\\:\\boldsymbol{\\mu},\\Sigma)=\\frac{1}{(2\\pi|\\Sigma|)^{\\frac{D}{2}}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)$\n",
    "\n",
    "### 0-2-1. 共分散行列の対角化 (§2.3)\n",
    "- $\\Sigma$ は対称行列なので正規直交系をなすような固有ベクトルをとれる.\n",
    "- $\\Sigma$ の固有値を $\\lambda_{1}, ...,\\lambda_{D}$ とし, 固有ベクトルを正規直交系をなすように $\\mathbf{u}_{1}, ...,\\mathbf{u}_{D}$ ととると $\\displaystyle \\Sigma=\\sum_{i=1}^{D}\\lambda_{i}\\mathbf{u}_{i}\\mathbf{u}_{i}^{T},\\:\\:\\:\\Sigma^{-1}=\\sum_{i=1}^{D}\\frac{1}{\\lambda_{i}}\\mathbf{u}_{i}\\mathbf{u}_{i}^{T}$\n",
    "    - このガウス分布の密度の等高面は, 中心が $\\boldsymbol{\\mu}$ の超単位球面を $\\mathbf{u}_{i}$ 方向に$\\displaystyle\\frac{1}{\\sqrt{\\lambda_{i}}}$倍に引き伸ばしたもの\n",
    "    \n",
    "### 0-2-2. モーメント (§2.3)\n",
    "- $\\mathbb{E}\\left[\\mathbf{x}\\right] = \\boldsymbol{\\mu},\\:\\:\\:\\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^{T}\\right] = \\boldsymbol{\\mu}\\boldsymbol{\\mu}^{T}+\\Sigma$\n",
    "\n",
    "### 0-2-3. 分割, 条件付き分布, 周辺化 (§2.3.1 - 2.3.2)\n",
    "- $\\displaystyle p\\left(\\left(\n",
    "    \\begin{array}{c}\n",
    "        \\mathbf{x}_{a}\\\\\n",
    "        \\mathbf{x}_{b}\\\\\n",
    "    \\end{array}\n",
    "    \\right)\\right)=\\mathcal{N}\\left(\\left(\n",
    "    \\begin{array}{c}\n",
    "        \\mathbf{x}_{a}\\\\\n",
    "        \\mathbf{x}_{b}\\\\\n",
    "    \\end{array} \n",
    "    \\right)\n",
    "    \\:\\left|\\:\\left(\n",
    "    \\begin{array}{c}    \n",
    "        \\boldsymbol{\\mu}_{a}\\\\\n",
    "        \\boldsymbol{\\mu}_{b}\\\\\n",
    "    \\end{array}  \n",
    "    \\right)\n",
    "    ,\n",
    "    \\left(\n",
    "    \\begin{array}{cc}\n",
    "        \\Sigma_{aa} & \\Sigma_{ab} \\\\\n",
    "        \\Sigma_{ba} & \\Sigma_{bb} \\\\        \n",
    "    \\end{array}  \n",
    "    \\right)\n",
    "    \\right.\\right)\n",
    "    $ とおく. ただし, $\n",
    "    \\left(\n",
    "    \\begin{array}{cc}\n",
    "        \\Sigma_{aa} & \\Sigma_{ab} \\\\\n",
    "        \\Sigma_{ba} & \\Sigma_{bb} \\\\        \n",
    "    \\end{array}  \n",
    "    \\right)\n",
    "    =\n",
    "    \\left(\n",
    "    \\begin{array}{cc}\n",
    "        \\Lambda_{aa} & \\Lambda_{ab} \\\\\n",
    "        \\Lambda_{ba} & \\Lambda_{bb} \\\\        \n",
    "    \\end{array}  \n",
    "    \\right)^{-1}\n",
    "    $ とする.\n",
    "    \n",
    "    - $\\mathbf{x}_{b}$ についての条件付きガウス分布:\n",
    "    $\\displaystyle p(\\mathbf{x}_{a}\\:|\\:\\mathbf{x}_{b}) = \\mathcal{N}\\left(\\mathbf{x}_{a}\\:|\\:\\boldsymbol{\\mu}_{a}-\\Lambda_{aa}^{-1}\\Lambda_{ab}(\\mathbf{x}_{b}-\\boldsymbol{\\mu}_{b}),\\:\\Lambda_{aa}^{-1}\\right)$\n",
    "    - $\\mathbf{x}_{b}$ について周辺化した分布:\n",
    "    $\\displaystyle p(\\mathbf{x}_{a}) = \\mathcal{N}\\left(\\mathbf{x}_{a}\\:|\\:\\boldsymbol{\\mu}_{a},\\:\\Sigma_{aa}\\right)$\n",
    "    \n",
    "### 0-2-4. 線形ガウスモデルにおける事前分布, 事後分布 (§2.3.3)\n",
    "- $\\displaystyle p(\\mathbf{x})=\\mathcal{N}(\\mathbf{x}\\:|\\:\\boldsymbol{\\mu},\\Lambda^{-1}),\\:\\:\\:\n",
    "p(\\mathbf{y}|\\mathbf{x})=\\mathcal{N}(\\mathbf{y}\\:|\\:\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1})\n",
    "$ とおくとき,\n",
    "    - 同時分布: $\\displaystyle p\\left(\\left(\n",
    "    \\begin{array}{c}\n",
    "        \\mathbf{x}\\\\\n",
    "        \\mathbf{y}\\\\\n",
    "    \\end{array}\n",
    "    \\right)\\right)=\\mathcal{N}\\left(\\left(\n",
    "    \\begin{array}{c}\n",
    "        \\mathbf{x}\\\\\n",
    "        \\mathbf{y}\\\\\n",
    "    \\end{array} \n",
    "    \\right)\n",
    "    \\:\\left|\\:\\left(\n",
    "    \\begin{array}{c}    \n",
    "        \\boldsymbol{\\mu}\\\\\n",
    "        \\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}\\\\\n",
    "    \\end{array}  \n",
    "    \\right)\n",
    "    ,\n",
    "    \\left(\n",
    "    \\begin{array}{cc}\n",
    "        \\Lambda^{-1} & \\Lambda^{-1}\\mathbf{A}^{T} \\\\\n",
    "        \\mathbf{A}\\Lambda^{-1} & \\mathbf{L}^{-1}+\\mathbf{A}\\Lambda^{-1}\\mathbf{A}^{T} \\\\        \n",
    "    \\end{array}  \n",
    "    \\right)\n",
    "    \\right.\\right) \n",
    "    $\n",
    "        - ただし $\n",
    "        \\displaystyle\n",
    "        \\left(\n",
    "        \\begin{array}{cc}\n",
    "            \\Lambda^{-1} & \\Lambda^{-1}\\mathbf{A}^{T} \\\\\n",
    "            \\mathbf{A}\\Lambda^{-1} & \\mathbf{L}^{-1}+\\mathbf{A}\\Lambda^{-1}\\mathbf{A}^{T} \\\\        \n",
    "        \\end{array}  \n",
    "        \\right) =\n",
    "        \\left(\n",
    "        \\begin{array}{cc}\n",
    "            \\Lambda+\\mathbf{A}^{T}\\mathbf{L}\\mathbf{A} & -\\mathbf{A}^{T}\\mathbf{L} \\\\\n",
    "            -\\mathbf{L}\\mathbf{A} & \\mathbf{L} \\\\        \n",
    "        \\end{array}  \n",
    "        \\right)^{-1}\n",
    "        $\n",
    "    - 周辺分布: $\\displaystyle p(\\mathbf{y})=\\mathcal{N}(\\mathbf{y}\\:|\\:\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b},\\:\\:\\mathbf{L}^{-1}+\\mathbf{A}\\Lambda^{-1}\\mathbf{A}^{T})$\n",
    "    - 条件付き分布: $\\displaystyle p(\\mathbf{x}|\\mathbf{y})=\\mathcal{N}(\\mathbf{x}\\:|\\:(\\Lambda+\\mathbf{A}^{T}\\mathbf{L}\\mathbf{A})^{-1}(\\mathbf{A}^{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\Lambda\\boldsymbol{\\mu}),\\:\\:(\\Lambda+\\mathbf{A}^{T}\\mathbf{L}\\mathbf{A})^{-1})$\n",
    "    \n",
    "### 0-2-5. 最尤推定とベイズ推論 (§2.3.4 - 2.3.6)\n",
    "\n",
    "- $\\displaystyle p(\\mathbf{x})=\\mathcal{N}(\\mathbf{x}\\:|\\:\\boldsymbol{\\mu},\\Sigma) = \\mathcal{N}(\\mathbf{x}\\:|\\:\\boldsymbol{\\mu},\\Lambda^{-1})$ とおくとき,\n",
    "\n",
    "    - 最尤推定\n",
    "        - $\\displaystyle\\boldsymbol{\\mu}_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_{n}\\;\\;\\;\\left(\\mathbb{E}\\left[\\boldsymbol{\\mu}_{\\mathrm{ML}}\\right]=\\boldsymbol{\\mu}\\right)$\n",
    "        - $\\displaystyle\\boldsymbol{\\Sigma}_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{\\mathrm{ML}})(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{\\mathrm{ML}})^{T}\\;\\;\\;\\left(\\mathbb{E}\\left[\\boldsymbol{\\Sigma}_{\\mathrm{ML}}\\right]=\\frac{N-1}{N}\\boldsymbol{\\Sigma}\\right)$\n",
    "        \n",
    "    - 精度行列は既知で平均が未知のときのベイズ推論\n",
    "        - 平均の事前分布: $p(\\boldsymbol{\\mu})=\\mathcal{N}\\left(\\boldsymbol{\\mu}\\:|\\:\\boldsymbol{\\mu}_{0},\\boldsymbol{\\Lambda}_{0}^{-1}\\right)$\n",
    "        - 平均の事後分布: $p(\\boldsymbol{\\mu}|\\mathbf{X})=\\mathcal{N}\\left(\\boldsymbol{\\mu}\\:|\\:\\left(N\\boldsymbol{\\Lambda}+\\boldsymbol{\\Lambda}_{0}\\right)^{-1}\\left(N\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}_{\\mathrm{ML}}+\\boldsymbol{\\Lambda}_{0}\\boldsymbol{\\mu}_{0}\\right),\\:\\left(N\\boldsymbol{\\Lambda}+\\boldsymbol{\\Lambda}_{0}\\right)^{-1}\\right)$\n",
    "        - 平均の点推定値: $\\boldsymbol{\\mu}_{\\mathrm{MAP}}=\\left(N\\boldsymbol{\\Lambda}+\\boldsymbol{\\Lambda}_{0}\\right)^{-1}\\left(N\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}_{\\mathrm{ML}}+\\boldsymbol{\\Lambda}_{0}\\boldsymbol{\\mu}_{0}\\right)$\n",
    "        \n",
    "    - 平均は既知で精度行列が未知のときのベイズ推論       \n",
    "        - 精度行列の事前分布: $p(\\boldsymbol{\\Lambda})=\\mathcal{W}\\left(\\boldsymbol{\\Lambda}\\:|\\:\\mathbf{W},\\nu\\right)$ (ウィシャート分布)\n",
    "        - 精度行列の事後分布: $\\displaystyle p(\\boldsymbol{\\Lambda}|\\mathbf{X})=\\mathcal{W}\\left(\\boldsymbol{\\Lambda}\\:\\left|\\:\\left(\\mathbf{W}^{-1}+\\sum_{n=1}^{N}(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{\\mathrm{ML}})(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{\\mathrm{ML}})^{T}\\right)^{-1},\\:\\:\\nu+N\\right.\\right)$\n",
    "    - 平均も精度行列も未知のときのベイズ推論       \n",
    "        - 平均と精度行列の事前分布: $p(\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}\\,|\\,\\boldsymbol{\\mu}_{0},\\beta,\\mathbf{W},\\nu)=\\mathcal{N}\\left(\\boldsymbol{\\mu}\\:|\\:\\boldsymbol{\\mu}_{0},(\\beta\\boldsymbol{\\Lambda})^{-1}\\right)\\mathcal{W}\\left(\\boldsymbol{\\Lambda}\\:|\\:\\mathbf{W},\\nu\\right)$ (ガウス-ウィシャート分布)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 2クラス分類のための識別関数\n",
    "\n",
    "## 1-1. 一般線形モデル (§4)\n",
    "\n",
    "- $y=f(\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x}))$ とおいて, $\\mathbf{w}$を最適化する. $f$は活性化関数.\n",
    "    - $f$ は線形でも非線形でもよい.\n",
    "    - 基底関数 $\\boldsymbol{\\phi}$ はあらかじめ決めておき, 動かさずに固定する.\n",
    "\n",
    "### 1-1-1. 一般線形モデル > 識別関数 > 最小二乗法 (§4.1.1 - 4.1.3)\n",
    "- 二乗和誤差関数 $\\displaystyle E(W)=\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^{T}(XW-T)\\}$ を最小化する.\n",
    "- 最適解は $W=(X^{T}X)^{-1}X^{T}T = X^{\\dagger}T$\n",
    "- $W\\mathbf{x}\\ge0$のとき$C_{1}$, $W\\mathbf{x}<0$のとき$C_{0}$と判定する.\n",
    "    - あまりうまくいかない\n",
    "    - そもそも最小二乗法とは目的変数の条件付き確率分布にガウス分布を仮定したときの最尤推定\n",
    "    - ここでは目的変数は二値変数であり, ガウス分布とはかけ離れているので当然のこと\n",
    "\n",
    "### 1-1-2. 一般線形モデル > 識別関数 > フィッシャーの線形判別 (§4.1.4 - 4.1.6)\n",
    "- フィッシャーの判別規準を最大化する.\n",
    "- 最適解は $\\mathbf{w}\\propto (\\mathbf{m}_{1}-\\mathbf{m}_{0})$\n",
    "- $\\mathbf{w}^{T}\\mathbf{x}\\ge -w_{0}$のとき$C_{1}$, $\\mathbf{w}^{T}\\mathbf{x}<-w_{0}$のとき$C_{0}$と判定する.\n",
    "    - しきい値$w_{0}$は$p(y|C_{k})$をモデル化して最尤推定などで求める\n",
    "    \n",
    "## 1-2. 決定木モデル (§14.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 2クラス分類のための識別モデルと生成モデル\n",
    "目標値が $t\\in\\{0,1\\}$ であり, $y(\\boldsymbol{\\phi}(\\mathbf{x}), \\mathbf{w})$ が $p(t=1|\\boldsymbol{\\phi}(\\mathbf{x}))$ を出力するような2クラス分類モデルを構成したいとき:\n",
    "\n",
    "## 2-1. 一般線形モデル (§4)\n",
    "\n",
    "- $y=f(\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x}))$ とおいて, $\\mathbf{w}$を最適化する. $f$は活性化関数.\n",
    "    - $f$ には非線形なものをえらぶ.\n",
    "    - 基底関数 $\\boldsymbol{\\phi}$ はあらかじめ決めておき, 動かさずに固定する.\n",
    "\n",
    "### 2-1-1. 一般線形モデル > 識別モデル (§4.3)\n",
    "\n",
    "#### 2-1-1-1. 一般線形モデル > 識別モデル > ロジスティック回帰  (§4.3.1 - 4.3.3)\n",
    "- $f$はロジスティックシグモイド. 理由は誤差関数の勾配が簡潔に書けるため(正準連結関数).\n",
    "- 最尤推定する場合の誤差関数(負の対数尤度比)はcross-entropy: $\\displaystyle E(\\mathbf{w})=-\\sum_{n=1}^{N}\\left(t_{n}\\ln y_{n}+(1-t_{n})\\ln(1-y_{n})\\right)$\n",
    "    - この $E(\\mathbf{w})$ は解析的には最小化できない(逐次的に小さくするしかない).\n",
    "    - $E(\\mathbf{w})$ の勾配は $\\displaystyle\\nabla_{\\mathbf{w}}E(\\mathbf{w})=\\sum_{n=1}^{N}(y_{n}-t_{n})\\boldsymbol{\\phi}$\n",
    "    - ニュートン・ラフソン法によって$\\mathbf{w}$を更新する方法は $\\mathbf{w}^{\\mathrm{new}}=\\mathbf{w}^{\\mathrm{old}}-H^{-1}\\nabla_{\\mathbf{w}}E(\\mathbf{w})=(\\Phi^{T}R\\Phi)^{-1}\\Phi^{T}R\\mathbf{z}$\n",
    "        - $\\Phi$ は $\\boldsymbol{\\phi}_{n}^{T}$ を行ベクトルにもつ$N\\times D$行列, $R$は $R_{nn}=y_{n}(1-y_{n})$ なる対角行列\n",
    "        - $\\mathbf{z}$ は $N$次元ベクトルで $\\mathbf{z}=\\Phi \\mathbf{w}^{\\mathrm{old}} - R^{-1}(\\textsf{y}-\\textsf{t})$\n",
    "    - 実際には正則化項を追加しないと過学習するので注意.\n",
    "\n",
    "#### 2-1-1-2, 一般線形モデル > 識別モデル > プロビット回帰  (§4.3.5)\n",
    "\n",
    "#### 2-1-1-3. 一般線形モデル > 識別モデル > ベイズロジスティック回帰  (§4.5)\n",
    "\n",
    "### 2-1-2. 一般線形モデル >  確率的生成モデル (§4.2)\n",
    "- $f$はロジスティックシグモイド.\n",
    "- 理由は $\\displaystyle p(t=1|\\mathbf{x})=\\displaystyle p(C_{1}|\\mathbf{x})=\\frac{1}{1+\\displaystyle\\frac{p(\\mathbf{x}|C_{1})p(C_{1})}{p(\\mathbf{x}|C_{0})p(C_{0})}}=\\sigma(a),\\:\\:\\:\\:a=\\ln\\frac{p(\\mathbf{x}|C_{1})p(C_{1})}{p(\\mathbf{x}|C_{0})p(C_{0})}$ とおけるため.\n",
    "- $a$を構成する事前確率を推定して線形モデルに書き直せばよい.\n",
    "\n",
    "#### 2-1-2-1. 一般線形モデル > 確率的生成モデル > 入力が連続値の場合  (§4.2.1 - 4.2.2)\n",
    "\n",
    "- 一般に $p(\\mathbf{x}|C_{k})$ が正準形指数型分布族のメンバー $(p(\\mathbf{x}|\\boldsymbol\\lambda_{k})=h(\\mathbf{x})g(\\boldsymbol\\lambda_{k})\\exp(\\boldsymbol\\lambda_{k}^{T}\\mathbf{x}))$ であるとき\n",
    "    - $s$をクラス間で共有された尺度パラメーターとすると $a=\\displaystyle\\frac{1}{s}(\\boldsymbol\\lambda_{1}-\\boldsymbol\\lambda_{0})^{T}\\mathbf{x}+\\ln\\frac{g(\\boldsymbol\\lambda_{1})p(C_{1})}{g(\\boldsymbol\\lambda_{0})p(C_{0})}$\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\displaystyle\\frac{1}{s}(\\boldsymbol\\lambda_{1}-\\boldsymbol\\lambda_{0}),\\:\\:\\:\\: w_{0}=\\ln\\frac{g(\\boldsymbol\\lambda_{1})p(C_{1})}{g(\\boldsymbol\\lambda_{0})p(C_{0})}$\n",
    "    \n",
    "        - あとは $\\boldsymbol\\lambda_{1},\\:\\boldsymbol\\lambda_{0},\\:p(C_{1}),\\:p(C_{0})$ を推定すればよい\n",
    "  \n",
    "  \n",
    "- とくに $p(\\mathbf{x}|C_{k})$ がガウス分布 $\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_{k},\\Sigma)$ のとき\n",
    "\n",
    "    - $\\displaystyle a=\\left(\\Sigma^{-1}(\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0})\\right)^{T}\\mathbf{x}-\\frac{1}{2}\\left(\n",
    "\\boldsymbol{\\mu}_{1}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{0}\n",
    "\\right)\n",
    "+\n",
    "\\ln \\frac{p(C_{1})}{p(C_{0})}$\n",
    "\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\Sigma^{-1}(\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0}),\\:\\:\\:\\:\n",
    "w_{0} = -\\frac{1}{2}\\left(\n",
    "\\boldsymbol{\\mu}_{1}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{0}^{T}\\Sigma^{-1}\\boldsymbol{\\mu}_{0}\n",
    "\\right)\n",
    "+\n",
    "\\ln \\frac{p(C_{1})}{p(C_{0})}$\n",
    "\n",
    "        - 最尤推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}}{N},\\:\\:\\:\\boldsymbol{\\mu}_{k}=\\frac{1}{N_{k}}\\sum_{n=1}^{N}t_{n}\\mathbf{x}_{k},\\:\\:\\:\\Sigma=\\frac{1}{N}\\sum_{k}\\sum_{n\\in C_{k}}(\\mathbf{x}_{k}-\\boldsymbol{\\mu}_{k})(\\mathbf{x}_{k}-\\boldsymbol{\\mu}_{k})^{T}$\n",
    "        - MAP推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}(N_{k}+1)}{N(N+1)}$\n",
    "  \n",
    "  \n",
    "- とくに $p(\\mathbf{x}|C_{k})$ がガウス分布であり, しかも各成分が独立であるとき $\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_{k},\\Sigma), \\:\\:\\sigma_{ij}=0 \\:\\mathrm{for}\\: i\\ne j$  \n",
    "👉`sklearn.naive_bayes.GaussianNB`\n",
    "\n",
    "#### 2-1-2-2. 一般線形モデル > 確率的生成モデル > 入力が離散値の場合  (§4.2.3)\n",
    "\n",
    "- 特徴値$x_{i}$が$(0,1)$の離散値をとり, $i\\ne j$に対して$x_{i},x_{j}$が独立なとき $\\displaystyle \\left( p(\\mathbf{x}|C_{k})=\\prod_{i=1}^{D} \\mu_{ki}^{x_{i}}(1-\\mu_{ki})^{1-x_{i}}\\right)$  \n",
    "👉`sklearn.naive_bayes.BernoulliNB`  \n",
    "    - $\\displaystyle a = \\sum_{i=1}^{D}\\ln\\frac{\\mu_{1i}(1-\\mu_{0i})}{\\mu_{0i}(1-\\mu_{1i})}x_{i}+\\sum_{i=1}^{D}\\ln\\frac{1-\\mu_{1i}}{1-\\mu_{0i}}+\\ln\\frac{p(C_{1})}{p(C_{0})}$\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\biggl\\{\\ln\\frac{\\mu_{1i}(1-\\mu_{0i})}{\\mu_{0i}(1-\\mu_{1i})}\\biggr\\},\\:\\:\\:\\:w_{0}=\\sum_{i=1}^{D}\\ln\\frac{1-\\mu_{1i}}{1-\\mu_{0i}}+\\ln\\frac{p(C_{1})}{p(C_{0})}$\n",
    "        - 最尤推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}}{N},\\:\\:\\:\\:\\mu_{ki}=\\frac{N_{ki}}{N_{k}}$\n",
    "        - MAP推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}(N_{k}+1)}{N(N+1)},\\:\\:\\:\\:\\mu_{ki}=\\frac{N_{ki}(N_{ki}+1)}{N_{k}(N_{k}+1)}$\n",
    "        \n",
    "        \n",
    "- 特徴値$x_{i}$が比例尺度であって, 0以上の整数の離散値をとり, $i\\ne j$に対して$x_{i},x_{j}$が独立なとき  \n",
    "👉`sklearn.naive_bayes.MultinomialNB`, `sklearn.naive_bayes.ComplementNB`\n",
    "\n",
    "- 特徴値$x_{i}$が$M_{i}$次元のone-hot vector $\\boldsymbol{\\phi}_{i}$で表現でき, $i\\ne j$に対して$x_{i},x_{j}$が独立なとき $\\displaystyle \\left( p(\\mathbf{x}|C_{k})=\\prod_{i=1}^{D}\\prod_{j=1}^{M_{i}} \\mu_{kij}^{\\phi_{ij}} \\right)$\n",
    "\n",
    "    - $\\displaystyle a = \\sum_{i=1}^{D}\\sum_{j=1}^{M_{i}}\\ln\\frac{\\mu_{1ij}}{\\mu_{0ij}}\\phi_{ij}+\\ln\\frac{p(C_{1})}{p(C_{0})} = \\sum_{i=1}^{D}\\mathbf{m}_{i}^{T}\\boldsymbol{\\phi}_{i}+\\ln\\frac{p(C_{1})}{p(C_{0})}\\:\\:\\:\\:\\left(\\mathbf{m}_{i}=\\biggl\\{ \\ln\\frac{\\mu_{1ij}}{\\mu_{0ij}}\\biggr\\}\\right)$\n",
    "    - つまり $\\displaystyle\\mathbf{w} = \\biggl\\{\\mathbf{m}_{i}\\biggr\\},\\:\\:\\:\\:w_{0}=\\ln\\frac{p(C_{1})}{p(C_{0})}$\n",
    "        - 最尤推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}}{N},\\:\\:\\:\\:\\mu_{kij}=\\frac{N_{kij}}{N_{ki}}$\n",
    "        - MAP推定解は $\\displaystyle p(C_{k})=\\frac{N_{k}(N_{k}+1)}{N(N+1)},\\:\\:\\:\\:\\mu_{kij}=\\frac{N_{kij}(N_{kij}+1)}{N_{ki}(N_{ki}+1)}$\n",
    "        \n",
    "        \n",
    "## 2-2. フィードフォワードネットワーク (§5)\n",
    "\n",
    "- $y=f(\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x}))$ とおいて, $\\mathbf{w}$と$\\boldsymbol{\\phi}$を最適化する(基底関数も動かす). $f$は非線形活性化関数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 2クラス分類のための識別モデルと生成モデル\n",
    "\n",
    "目標値が $t\\in\\{-1,1\\}$ であり, $y(\\mathbf{x}, \\mathbf{w})$ が $-1\\le y \\le 1$ を出力するような2クラス分類モデルを構成したいとき:\n",
    "\n",
    "- 活性化関数をロジスティックシグモイドから $\\tanh$ に変更する."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 多クラス分類のための識別関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 多クラス分類のための識別モデルと生成モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 多クラス分類のための識別モデルと生成モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 目標値がスカラーのときの回帰\n",
    "## 7-1. 一般線形モデル\n",
    "- $y=\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x})$ とおいて, $\\mathbf{w}$を最適化する.\n",
    "    - 基底関数 $\\boldsymbol{\\phi}$ はあらかじめ決めておき, 動かさずに固定する.\n",
    "    - 基底関数を $\\boldsymbol{\\phi}_{j}(\\mathbf{x})=x^{j}$ とすれば多項式フィッテイングとなる.\n",
    "\n",
    "### 7-1. 一般線形モデル > 予測値を点推定\n",
    "- 二乗和誤差関数 $\\displaystyle E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^{N}(t_{n}-\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x}))^{2}$ を最小化する.\n",
    "- 最適解は $\\mathbf{w}=\\Phi^{\\dagger}\\textsf{t}$\n",
    "- 予測値は $t=(\\Phi^{\\dagger}\\textsf{t})^{T}\\mathbf{x}$\n",
    "    - $\\Phi^{\\dagger}$ は $\\Phi$ のムーア・ペンローズ擬似逆行列で, $\\Phi^{\\dagger}=(\\Phi^{T}\\Phi)^{-1}\\Phi^{T}$\n",
    "    - $\\textsf{t}$ は $N$ 次元ベクトルで, $\\textsf{t}=(t_{1} ... t_{n})^{T}$\n",
    "\n",
    "### 7-2. 一般線形モデル > 予測値を確率的に推定\n",
    "#### 7-2-1. 一般線形モデル > 予測値を確率的に推定 > 頻度主義\n",
    "- 目標値 $t$ にガウスノイズを仮定する $\\displaystyle \\left(t=\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x})+\\epsilon,\\;\\;p(\\epsilon)=\\mathcal{N}(\\epsilon|0,\\beta^{-1})\\right)$.\n",
    "- 次の(1)(2)の結論は同じ:\n",
    "    - (1) 最尤推定する.\n",
    "    - (2) 二乗和誤差関数 $\\displaystyle E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^{N}(t_{n}-\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x}))^{2}$を最小化する.\n",
    "- 最適解は $\\displaystyle\\mathbf{w}_{\\mathrm{ML}}=\\Phi^{\\dagger}\\textsf{t},\\:\\:\\:\\:\\beta^{-1}_{\\mathrm{ML}}=\\frac{1}{N}\\sum_{n=1}^{N}(t_{n}-\\mathbf{w}^{T}_{\\mathrm{ML}}\\boldsymbol{\\phi}(\\mathbf{x}))^{2}=\\frac{1}{N}\\sum_{n=1}^{N}(t_{n}-(\\Phi^{\\dagger}\\textsf{t})^{T}\\mathbf{x})^{2}$\n",
    "- 予測分布は $\\displaystyle p(t|\\mathbf{x},\\mathbf{w}_{\\mathrm{ML}},\\beta^{-1}_{\\mathrm{ML}})=\\mathcal{N}(t|w_{\\mathrm{ML}}^{T}\\mathbf{x},\\beta^{-1}_{\\mathrm{ML}}) = \\mathcal{N}\\left(t\\:\\left|\\:(\\Phi^{\\dagger}\\textsf{t})^{T}\\mathbf{x}, \\frac{1}{N}\\sum_{n=1}^{N}(t_{n}-(\\Phi^{\\dagger}\\textsf{t})^{T}\\mathbf{x})^{2}\\right.\\right)$\n",
    "- 回帰関数は $(\\Phi^{\\dagger}\\textsf{t})^{T}\\mathbf{x}=\\mathbb{E}_{t}\\left[t|\\mathbf{x}\\right]$ をみたす.\n",
    "    - $\\Phi^{\\dagger}$ は $\\Phi$ のムーア・ペンローズ擬似逆行列で, $\\Phi^{\\dagger}=(\\Phi^{T}\\Phi)^{-1}\\Phi^{T}$\n",
    "    - $\\textsf{t}$ は $N$ 次元ベクトルで, $\\textsf{t}=(t_{1} ... t_{n})^{T}$\n",
    "    - $\\beta^{-1}_{\\mathrm{ML}}$ は回帰関数まわりでの残差分散となっている.\n",
    "    \n",
    "#### 7-2-2. 一般線形モデル > 予測値を確率的に推定 > ベイズ線形回帰\n",
    "- 目標値 $t$ にガウスノイズを仮定する $\\displaystyle \\left(t=\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x})+\\epsilon,\\;\\;p(\\epsilon)=\\mathcal{N}(\\epsilon|0,\\beta^{-1})\\right)$.\n",
    "- 次の(1)(2)の結論は同じ:\n",
    "    - (1) $\\mathbf{w}$ の事前分布を $p(\\mathbf{w}|\\alpha)=\\mathcal{N}(\\mathbf{w}|\\mathbf{0},\\alpha^{-1}I)$ とおいてMAP推定する.\n",
    "    - (2) L2ノルムで正則化した二乗和誤差関数 $\\displaystyle E(\\mathbf{w},\\lambda)=\\frac{1}{2}\\sum_{n=1}^{N}(t_{n}-\\mathbf{w}^{T}\\boldsymbol{\\phi}(\\mathbf{x}))^{2} + \\frac{\\lambda}{2}\\sum_{j=1}^{D}w_{j}^{2}$ を最小化する.\n",
    "        - (1)は(2)で $\\displaystyle\\lambda = \\frac{\\alpha}{\\beta}$ とおいたものに帰着する.\n",
    "- 最適解は $\\displaystyle\\mathbf{w}_{\\mathrm{MAP}}=(\\Phi^{\\dagger}+\\lambda I)\\textsf{t}$\n",
    "- 予測分布は $\\displaystyle p(t|\\textsf{t},\\mathbf{x},\\alpha,\\beta)=\\mathcal{N}(t|w_{\\mathrm{ML}}^{T}\\mathbf{x},\\beta^{-1}_{\\mathrm{ML}}) = \\mathcal{N}\\left(t\\:\\left|\\:(\\Phi^{\\dagger}\\textsf{t})^{T}\\mathbf{x}, \\frac{1}{N}\\sum_{n=1}^{N}(t_{n}-(\\Phi^{\\dagger}\\textsf{t})^{T}\\mathbf{x})^{2}\\right.\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 目標値がベクトルのときの回帰\n",
    "## 8-1. 一般線形モデル\n",
    "- $y=W^{T}\\boldsymbol{\\phi}(\\mathbf{x})$ とおいて, $W$を最適化する.\n",
    "    - 基底関数 $\\boldsymbol{\\phi}$ はあらかじめ決めておき, 動かさずに固定する.\n",
    "\n",
    "### 8-1. 一般線形モデル > 目標値を決定論的に扱う\n",
    "- 目標値 $t$ を決定論的に扱うとき,\n",
    "    - 二乗和誤差関数 $\\displaystyle E(W)=\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^{T}(XW-T)\\}$ を最小化する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
